---
title: Invited Speakers
layout: single
excerpt: "AIST-2023 Invited Speakers."
permalink: /program/invited/
sidebar: 
    nav: program
---

The following speakers have graciously accepted to give keynotes at AIST-2023.<br>

 
## Samuel Horvath

<figure>
  <a href="https://mbzuai.ac.ae/study/faculty/samuel-horvath/"><img length="100" src="/assets/images/sam_headshot.jpeg"></a>
  <figcaption><strong><a href="https://mbzuai.ac.ae/study/faculty/samuel-horvath/">Samuel Horvath</a></strong> is an assistant professor of Machine Learning at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI). Before joining MBZUAI, he completed his MS and Ph.D. in statistics at King Abdullah University of Science and Technology (KAUST) advised by Professor Peter Richtárik. Earlier in his academic journey, Samuel was an undergraduate student in Financial Mathematics at Comenius University.
In his research, Samuel focuses on providing a fundamental understanding of how distributed and federated training algorithms function and their interactions with various sources of heterogeneity. These sources include system-level computing infrastructure variability and training data statistical variability. Driven by these theoretical insights, Samuel aims to create efficient and practical distributed and federated training algorithms.
Samuel's broad interests lie in distributed, collaborative, and efficient on-device ML.
</figcaption>
</figure>

<b>Towards Real-World Federated Learning: Addressing Client Heterogeneity and Model Size</b> <br/> <br/>
<b>Abstract:</b> In this talk, I will introduce federated learning and discuss two recent approaches for addressing the challenges of client heterogeneity and model size in federated learning.
In the first part of the talk, I will introduce federated learning. I will discuss the motivation for federated learning, the key challenges, and some of the existing approaches.
In the second part of the talk, I will discuss the FjORD framework. FjORD is a framework for addressing the problem of client heterogeneity in federated learning. FjORD uses Ordered Dropout to gradually prune the model width without retraining, enabling clients with different capabilities to participate by tailoring the model width to the client's capabilities.
In the third part of the talk, I will discuss the Maestro framework. Maestro is a framework for addressing the problem of model size in federated learning. Maestro uses a technique called trainable low-rank layers to compress the model without sacrificing accuracy.
I will conclude the talk by discussing the future of federated learning.

## Hakim Hacid
<figure>
  <a href="https://scholar.google.ae/citations?user=62FX_zEAAAAJ&hl=en"><img length="100" src="/assets/images/hakim.jpg"></a>
  <figcaption><strong><a href="https://scholar.google.ae/citations?user=62FX_zEAAAAJ&hl=en">Hakim Hacid</a></strong> is the principal researcher at the AI cross-centre unit at the Technology Innovation Institute (TII), a leading scientific research centre based in the United Arab Emirates as well as an Honorary Professor at Macquarie University in Sydney, Australia. Prior to joining TII, he was an associate professor at Zayed University and contributed to research in the areas of data analysis, information retrieval and security. He also served as chairman of the Department of Computer Science and Applied Technology. Dr. Hacid has authored over 60 research papers published in leading journals and conferences and holds several industrial patents. His research interests include databases, data mining and analysis, programming, web information systems, natural language processing and security. Hakim Hacid received his PhD in data mining/databases from the University of Lyon, France. He also obtained a double master's degree in computer science (research and professional master's) from the same university.</figcaption>
</figure>

<b>Towards Edge AI: Principles, current state, and perspectives</b> <br/> <br/>
<b>Abstract:</b> The artificial intelligence (AI) community has invested heavily in developing techniques that can digest very large amounts of data to extract valuable information and knowledge. Most techniques, particularly deep learning models, require large amounts of computing and storage power, making them suitable for cloud-based environments. The intelligence is therefore remote from the end user, raising concerns about, for example, data privacy and latency. Edge AI addresses some of the problems inherent in the cloud and focuses on best practices, architectures and processes for extending data AI outside the cloud. Edge AI brings AI closer to the end user and uses, for example, fewer communication resources, as processing is performed directly on the edge device. This presentation will introduce edge AI and give an overview of existing work and potential future contributions.


## Artem Shelmanov

<figure>
  <a href="https://scholar.google.com/citations?user=-zFR1g0AAAAJ&hl"><img width="300" src="/assets/images/artem.jpg"></a>
  <figcaption><strong><a href="https://scholar.google.com/citations?user=-zFR1g0AAAAJ&hl">Artem Shelmanov</a></strong> graduated from NRU MEPhI in 2012 and received his PhD degree in 2015 from Russian Academy of Sciences (FRC CSC RAS). His doctoral thesis is devoted to the semantic parsing of natural language texts and its applications in information retrieval.
He worked as a PostDoc in Skoltech, as a team leader of an NLP analytics group in SAS CIS, and as a leading research scientist in AIRI, where he led a research group focused on active learning and uncertainty estimation for NLP models. Currently, he is a Sr. Research Scientist in the NLP department at the Mohamed bin Zayed University of Artificial Intelligence: MBZUAI in UAE. 
His primary research interests encompass uncertainty estimation for LLMs, the advancement of trustworthy AI, weakly-supervised learning, and memory-augmented neural models.</figcaption>
</figure>

<b>Safety of Deploying NLP Models: Uncertainty Quantification of Generative LLMs</b> <br/> <br/>
<b>Abstract:</b> When deploying a machine learning (ML) model in practice, care should be taken to look beyond prediction performance metrics such as accuracy or F1. We should ensure also that it safe to use ML-based applications. This entails that applications should be evaluated along other critical dimensions such as reliability and fairness. The widespread deployment of large language models (LLMs) has made ML-based applications even more vulnerable to risks of causing various forms of harm to users. While streamline research effort has been devoted to the “alignment” via various forms of fine-tuning and to fact checking of the generated output, in this talk, we focus on uncertainty quantification as an effective approach to another important problem of LLMs. Models often ``hallucinate'', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods could be used to detect unreliable generations unlocking the safer and more responsible use of LLMs in practice. UE methods for generative LLMs are a subject of bleeding-edge research, which is currently quite scarce and scattered. We systemize these efforts, discuss common caveats, and provide suggestions for the development of novel techniques in this area.
