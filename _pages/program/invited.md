---
title: Keynote Speakers
layout: single
excerpt: "AIST-2023 Keynote Speakers."
permalink: /program/keynote/
sidebar: 
    nav: program
---

The following speakers have graciously accepted to give keynotes at AIST-2023.<br>

 
## Samuel Horvath

<figure>
  <a href="https://mbzuai.ac.ae/study/faculty/samuel-horvath/"><img length="100" src="/assets/images/sam_headshot.jpeg"></a>
  <figcaption><strong><a href="https://mbzuai.ac.ae/study/faculty/samuel-horvath/">Samuel Horvath</a></strong> is an assistant professor of Machine Learning at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI). Before joining MBZUAI, he completed his MS and Ph.D. in statistics at King Abdullah University of Science and Technology (KAUST) advised by Professor Peter Richtárik. Earlier in his academic journey, Samuel was an undergraduate student in Financial Mathematics at Comenius University.
In his research, Samuel focuses on providing a fundamental understanding of how distributed and federated training algorithms function and their interactions with various sources of heterogeneity. These sources include system-level computing infrastructure variability and training data statistical variability. Driven by these theoretical insights, Samuel aims to create efficient and practical distributed and federated training algorithms.
Samuel's broad interests lie in distributed, collaborative, and efficient on-device ML.
</figcaption>
</figure>

<b>Towards Real-World Federated Learning: Addressing Client Heterogeneity and Model Size</b> <br/> <br/>
<b>Abstract:</b> In this talk, I will introduce federated learning and discuss two recent approaches for addressing the challenges of client heterogeneity and model size in federated learning.
In the first part of the talk, I will introduce federated learning. I will discuss the motivation for federated learning, the key challenges, and some of the existing approaches.
In the second part of the talk, I will discuss the FjORD framework. FjORD is a framework for addressing the problem of client heterogeneity in federated learning. FjORD uses Ordered Dropout to gradually prune the model width without retraining, enabling clients with different capabilities to participate by tailoring the model width to the client's capabilities.
In the third part of the talk, I will discuss the Maestro framework. Maestro is a framework for addressing the problem of model size in federated learning. Maestro uses a technique called trainable low-rank layers to compress the model without sacrificing accuracy.
I will conclude the talk by discussing the future of federated learning.

## Hakim Hacid
<figure>
  <a href="https://scholar.google.ae/citations?user=62FX_zEAAAAJ&hl=en"><img length="100" src="/assets/images/hakim.jpg"></a>
  <figcaption><strong><a href="https://scholar.google.ae/citations?user=62FX_zEAAAAJ&hl=en">Hakim Hacid</a></strong> is the principal researcher at the AI cross-centre unit at the Technology Innovation Institute (TII), a leading scientific research centre based in the United Arab Emirates as well as an Honorary Professor at Macquarie University in Sydney, Australia. Prior to joining TII, he was an associate professor at Zayed University and contributed to research in the areas of data analysis, information retrieval and security. He also served as chairman of the Department of Computer Science and Applied Technology. Dr. Hacid has authored over 60 research papers published in leading journals and conferences and holds several industrial patents. His research interests include databases, data mining and analysis, programming, web information systems, natural language processing and security. Hakim Hacid received his PhD in data mining/databases from the University of Lyon, France. He also obtained a double master's degree in computer science (research and professional master's) from the same university.</figcaption>
</figure>

<b>Towards Edge AI: Principles, current state, and perspectives</b> <br/> <br/>
<b>Abstract:</b> The artificial intelligence (AI) community has invested heavily in developing techniques that can digest very large amounts of data to extract valuable information and knowledge. Most techniques, particularly deep learning models, require large amounts of computing and storage power, making them suitable for cloud-based environments. The intelligence is therefore remote from the end user, raising concerns about, for example, data privacy and latency. Edge AI addresses some of the problems inherent in the cloud and focuses on best practices, architectures and processes for extending data AI outside the cloud. Edge AI brings AI closer to the end user and uses, for example, fewer communication resources, as processing is performed directly on the edge device. This presentation will introduce edge AI and give an overview of existing work and potential future contributions.


## Artem Shelmanov

<figure>
  <a href="https://scholar.google.com/citations?user=-zFR1g0AAAAJ&hl"><img width="300" src="/assets/images/artem.jpg"></a>
  <figcaption><strong><a href="https://scholar.google.com/citations?user=-zFR1g0AAAAJ&hl">Artem Shelmanov</a></strong> graduated from NRU MEPhI in 2012 and received his PhD degree in 2015 from Russian Academy of Sciences (FRC CSC RAS). His doctoral thesis is devoted to the semantic parsing of natural language texts and its applications in information retrieval.
He worked as a PostDoc in Skoltech, as a team leader of an NLP analytics group in SAS CIS, and as a leading research scientist in AIRI, where he led a research group focused on active learning and uncertainty estimation for NLP models. Currently, he is a Sr. Research Scientist in the NLP department at the Mohamed bin Zayed University of Artificial Intelligence: MBZUAI in UAE. 
His primary research interests encompass uncertainty estimation for LLMs, the advancement of trustworthy AI, weakly-supervised learning, and memory-augmented neural models.</figcaption>
</figure>

<b>Safety of Deploying NLP Models: Uncertainty Quantification of Generative LLMs</b> <br/> <br/>
<b>Abstract:</b> When deploying a machine learning (ML) model in practice, care should be taken to look beyond prediction performance metrics such as accuracy or F1. We should ensure also that it safe to use ML-based applications. This entails that applications should be evaluated along other critical dimensions such as reliability and fairness. The widespread deployment of large language models (LLMs) has made ML-based applications even more vulnerable to risks of causing various forms of harm to users. While streamline research effort has been devoted to the "alignment" via various forms of fine-tuning and to fact checking of the generated output, in this talk, we focus on uncertainty quantification as an effective approach to another important problem of LLMs. Models often "hallucinate", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods could be used to detect unreliable generations unlocking the safer and more responsible use of LLMs in practice. UE methods for generative LLMs are a subject of bleeding-edge research, which is currently quite scarce and scattered. We systemize these efforts, discuss common caveats, and provide suggestions for the development of novel techniques in this area.


## Narine Sarvazyan

<figure>
  <a href="https://people.aua.am/team_member/narine-sarvazyan-phd/"><img width="300" src="/assets/images/nina.jpg"></a>
  <figcaption><strong><a href="https://people.aua.am/team_member/narine-sarvazyan-phd/">Narine Sarvazyan</a></strong> received her bachelor's and master’s degrees in physics and biophysics
from Lomonosov Moscow State University (1980–1986), followed by a doctoral degree in biological sciences from the Institute of Experimental Biology, Armenia (1991). Upon moving to the United States, she joined the faculty of Texas Tech University (1994-2004) and later George Washington University (2004-present). She is actively involved in several projects in Armenia, serving as the William Frazer Endowed Professor and Director of the Akian’s Bioscience lab at the American University of Armenia. Over the past three decades, Dr. Sarvazyan’s research efforts have been continuously supported by the National Institutes of Health, the National Science Foundation, the American Heart Association, and other major funding agencies. Her research focuses on the basic mechanisms of cardiac arrhythmias, cardiotoxicity of cancer drugs and environmental contaminants, stem cell therapies, and new imaging modalities to visualize pathophysiological tissue states. She has been extensively published across various renowned peer-reviewed journals and holds several patents to her name. Dr. Sarvazyan is a recipient of the prestigious Established Investigator Award from the American Heart Association, the Fulbright Scholar Award from the US State Department, and the ERA Chair Award from the European Union.</figcaption>
</figure>

<b>Decoding Hyperspectral Imaging: From Basic Principles to Medical Applications</b> <br/> <br/>
<b>Abstract:</b> Over the past few decades, the application of hyperspectral imaging (HSI) has significantly expanded, finding widespread use in areas such as satellite imaging, agriculture, the food industry, and medicine. What sets HSI apart is its capacity to acquire complete spectral data from every pixel of an image. Each HSI dataset is a collection of individual images across numerous spectral bands and/or varied lighting conditions. A distinctive element of HSI is that, unlike grayscale or color images – where each pixel contains one or three to four color channels respectively – HSI captures hundreds of spectral bands for every pixel. Hence, the output from HSI is essentially a three- or four-dimensional dataset, with two dimensions representing spatial axes and the rest providing spectral values. Each dimension typically encompasses hundreds of individual values, so the massive amount of information collected by HSI hardware presents a great opportunity to apply ML and AI tools for data analysis. During this keynote presentation, the speaker will overview the fundamental principles of HSI technology including examples from her own projects. The goal will be to illustrate both the immense promise of HSI in revealing previously unseen surgical targets, as well as challenges posed by the high-dimensionality of HSI data. The key considerations for automatic processing and analysis of HSI data for medical use will also be touched upon. These include preserving the original spectral detail of an image to prevent the loss of information, ensuring processing efficiency for real-time application in a clinical environment, and managing the demands on processing power to ensure broader implementation.


## Muhammad Shahid Iqbal Malik

<figure>
  <a href="https://www.hse.ru/en/org/persons/785341107"><img width="300" src="/assets/images/mh.jpg"></a>
  <figcaption><strong><a href="https://www.hse.ru/en/org/persons/785341107">Muhammad Shahid Iqbal Malik</a></strong> is currently a Postdoc Fellow in the Lab for Models and Methods of Computational Pragmatics, National Research University HSE, Moscow, Russia. Dr. Malik received his Master degree in Computer Engineering (2011), followed by a Doctoral degree in Data Mining (2018) from International Islamic University, Islamabad, Pakistan. Previously, he served more than 3 years as an Assistant Professor at CUST University, Islamabad and 4 years as a Lecturer at Comsats University Islamabad, Pakistan. In addition, he served 12 years in HVAC industry, Islamabad and developed several embedded systems solutions for Air-conditioning systems. Dr. Malik authored 23 research papers published in leading International Journals and Conferences. His research interests include Social Media Mining, Natural Language Processing, Predictive Analytics and Social Computing.</figcaption>
</figure>

<b>Threatening Content and Target Identification in low-resource languages using NLP Techniques</b> <br/> <br/>
<b>Abstract:</b> Identification of threatening text on social media platforms is a challenging task. Contrary to the high-resource languages, the Urdu language has very limited such approaches and the benchmark approach has an issue of inappropriate data annotation. Therefore, we present robust threatening content and target identification as a hierarchical classification model for Urdu tweets. This study investigates the potential of the Urdu-BERT (Bidirectional Encoder Representations from Transformer) language model to learn universal contextualized representations aiming to showcase its usefulness for binary classification tasks of threatening content and target identification. We propose to exploit a pre-trained Urdu-BERT as a transfer learning model after fine-tuning its parameters on a newly designed Urdu corpus from the Twitter platform. The proposed dataset contains 2,400 tweets manually annotated as threatening or non-threatening at the first level and threatening tweets are further categorized into individual or group at the second level. The performance of fine-tuned Urdu-BERT is compared with the benchmark study and other feature models. Experimental results show that the fine-tuned Urdu-BERT model achieves state-of-the-art performance by obtaining 87.5% accuracy and 87.8% F1-score for threatening content identification and 82.5% accuracy and 83.2% F1-score for target identification task. Furthermore, the proposed model outperforms the benchmark study.    


